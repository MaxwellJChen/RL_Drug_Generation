{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from Model.SURGE import SURGE\n",
    "from Reinforcement_Learning.mol_env import vectorized_mol_env\n",
    "from Model.graph_embedding import batch_from_states\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "import wandb\n",
    "import copy\n",
    "import rdkit.Chem.QED as QED\n",
    "import sys\n",
    "import os\n",
    "from rdkit.Chem import RDConfig # SAS\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SURGE()\n",
    "max_steps = 200\n",
    "num_envs = 4\n",
    "env = vectorized_mol_env(num_envs = num_envs, max_steps = max_steps) # Vectorized molecular environment\n",
    "lr = 0.02\n",
    "optimizer = Adam(lr = lr, params = model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f32500a4953f295"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "eps = np.finfo(np.float32).eps.item() # Small constant to decrease numerical instability\n",
    "num_episodes = 500"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ad436df1363c9c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_datetime = datetime.datetime.now()\n",
    "current_time = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "wandb.init(\n",
    "    project = 'RL_Drug_Generation',\n",
    "    name= f'REINFORCE---{current_time}',\n",
    "    config={\n",
    "        'lr': lr,\n",
    "        'architecture': str(model),\n",
    "        'episodes': num_episodes,\n",
    "        'gamma': gamma,\n",
    "        'num_envs': num_envs\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4af6d57c08a0eaec"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def evaluate(saved_states, saved_actions):\n",
    "    \"\"\"\n",
    "    This methods evaluates the molecules generated during the episode and returns diversity, novelty, and validity.\n",
    "    It also analyzes the molecule metrics of drug-likeness (QED), synthetic accessibility (SAS), and size.\n",
    "    It only analyzes the finished molecules (i.e., after SURGE tells generation to terminate).\n",
    "    \"\"\"\n",
    "    \n",
    "    flattened_states = []\n",
    "    flattened_t = []\n",
    "    for i in range(saved_states.shape[1]):\n",
    "        states_col = saved_states[:, i].flatten().tolist()\n",
    "        t_col = saved_actions['t'][:, i].flatten().tolist()\n",
    "        if i == 0:\n",
    "            flattened_states = states_col\n",
    "            flattened_t = t_col\n",
    "        else:\n",
    "            flattened_states += states_col\n",
    "            flattened_t += t_col\n",
    "    \n",
    "    idx = [i for i, t in enumerate(flattened_t) if t == 1]\n",
    "    finished_states = [flattened_states[idx[i]] for i in range(len(idx))]\n",
    "    \n",
    "    \n",
    "    # QED: From 0 to 1 where 1 is the most drug-like\n",
    "    # SAS: From 1 to 10 where 1 is the easiest to synthesize\n",
    "    metric_names = ['Average Size', 'Average QED', 'Average SAS']\n",
    "    num_mols = len(finished_states)\n",
    "    metrics = dict()\n",
    "    for name in metric_names:\n",
    "        metrics[name] = 0\n",
    "        \n",
    "    for i in range(num_mols):\n",
    "        mol_size = finished_states[i].GetNumAtoms()\n",
    "        if mol_size == 1:\n",
    "            metrics['Average SAS'] = 0\n",
    "        else:\n",
    "            metrics['Average SAS'] += sascorer.calculateScore(finished_states[i])\n",
    "        metrics['Average Size'] += mol_size\n",
    "        metrics['Average QED'] += QED.weights_mean(finished_states[i])\n",
    "    \n",
    "    for name in metric_names:\n",
    "        metrics[name] /= num_mols\n",
    "    \n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T04:49:16.853101Z",
     "start_time": "2023-10-22T04:49:16.840246Z"
    }
   },
   "id": "129d2caf1577df9e"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def calc_returns(saved_rewards, gamma):\n",
    "    \"\"\"\n",
    "    Given multidimensional matrix of rewards, computes the returns.\n",
    "    \"\"\"\n",
    "    prev_row = []\n",
    "    for reward_idx in reversed(range(saved_rewards.shape[0])):\n",
    "        cur_row = saved_rewards[reward_idx, :]\n",
    "        if reward_idx == saved_rewards.shape[0] - 1:\n",
    "            all_returns = cur_row\n",
    "            prev_row = cur_row\n",
    "        else:\n",
    "            all_returns = torch.vstack((prev_row * gamma + cur_row, all_returns))\n",
    "            prev_row *= gamma\n",
    "            prev_row += cur_row\n",
    "    return all_returns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T04:49:17.834907Z",
     "start_time": "2023-10-22T04:49:17.828767Z"
    }
   },
   "id": "d259874b09dfefef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_reward_model = copy.deepcopy(model)\n",
    "best_reward = 0\n",
    "best_episode = 0\n",
    "for episode in range(1, num_episodes + 1):\n",
    "\n",
    "    # Reset environment after each episode\n",
    "    states = env.reset()\n",
    "\n",
    "    # Episode loggers\n",
    "    keys = ['t', 'nmol', 'nfull', 'b']\n",
    "    saved_actions = {'t': [], 'nmol': [], 'nfull': [], 'b': []}\n",
    "    saved_log_probs = {'t': [], 'nmol': [], 'nfull': [], 'b': []}\n",
    "    saved_rewards = []\n",
    "    saved_states = np.array(states, dtype = object)\n",
    "\n",
    "    # Episode computation\n",
    "    pbar = trange(max_steps, unit=\"steps\")\n",
    "    for step in pbar:\n",
    "        pbar.set_description(f\"Episode {episode}\")\n",
    "        \n",
    "        # Compute actions and log probabilities\n",
    "        batch = batch_from_states(states)\n",
    "        actions, log_probs = model.act(batch)\n",
    "\n",
    "        # Record in episode loggers\n",
    "        for key in keys:\n",
    "            if step == 0:\n",
    "                saved_actions[key] = actions[key]\n",
    "                saved_log_probs[key] = log_probs[key]\n",
    "            else:\n",
    "                saved_actions[key] = np.vstack((saved_actions[key], actions[key]))\n",
    "                saved_log_probs[key] = torch.vstack((saved_log_probs[key], log_probs[key]))\n",
    "\n",
    "        # Take a step in environment\n",
    "        states, rewards, valids, timestep = env.step(actions['t'], actions['nmol'], actions['nfull'], actions['b'])\n",
    "        saved_states = np.vstack((saved_states, states))\n",
    "\n",
    "        # Record rewards\n",
    "        if step == 0:\n",
    "            saved_rewards = torch.tensor(rewards)\n",
    "        else:\n",
    "            saved_rewards = torch.vstack((saved_rewards, torch.tensor(rewards)))\n",
    "\n",
    "    # Loss calculation and gradient ascent\n",
    "    cumulative_reward = torch.sum(saved_rewards) / num_envs\n",
    "\n",
    "    # Returns\n",
    "    all_returns = calc_returns(saved_rewards, gamma)\n",
    "    returns = torch.zeros(num_envs)\n",
    "    all_returns = (all_returns - all_returns.mean()) / (all_returns.std() + eps) # Normalize returns for better stability\n",
    "    \n",
    "    # Calculate loss\n",
    "    all_loss = dict()\n",
    "    cumulative_loss = 0\n",
    "    for key in keys:\n",
    "        # Find the average loss among vectorized environments for each SURGE component\n",
    "        individual_loss = -1 * all_returns * saved_log_probs[key] / num_envs\n",
    "        cumulative_loss += torch.sum(individual_loss)\n",
    "        all_loss[key] = torch.sum(individual_loss)\n",
    "\n",
    "    # Perform gradient ascent\n",
    "    optimizer.zero_grad()\n",
    "    cumulative_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    metrics = evaluate(saved_states, saved_actions)\n",
    "    metrics['Cumulative Reward'] = cumulative_reward.item()\n",
    "    metrics['Cumulative Loss'] = cumulative_loss.item()\n",
    "    metrics['Nmol Loss'] = all_loss['nmol'].item()\n",
    "    metrics['Nfull Loss'] = all_loss['nfull'].item()\n",
    "    metrics['Bond Loss'] = all_loss['b'].item()\n",
    "    metrics['Termination Loss'] = all_loss['t'].item()\n",
    "    \n",
    "    for key in metrics.keys():\n",
    "        print(f\"{key}: {metrics[key]}\")\n",
    "    \n",
    "    if cumulative_reward > best_reward:\n",
    "        best_reward = cumulative_reward\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_episode = episode\n",
    "    \n",
    "    wandb.log(metrics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bda2bbdddefc344"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778c374f34d1064c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
