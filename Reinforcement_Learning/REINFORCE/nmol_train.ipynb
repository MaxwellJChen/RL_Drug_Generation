{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from Model.SURGE import SURGE\n",
    "from Reinforcement_Learning.mol_env import vectorized_mol_env\n",
    "from Model.graph_embedding import batch_from_states\n",
    "import numpy as np\n",
    "import datetime\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SURGE()\n",
    "max_steps = 200\n",
    "num_envs = 4\n",
    "env = vectorized_mol_env(num_envs = num_envs, max_steps = max_steps) # Vectorized molecular environment\n",
    "lr = 0.02\n",
    "optimizer = Adam(lr = lr, params = model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf9f5c9062bf83bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "eps = np.finfo(np.float32).eps.item() # Small constant to decrease numerical instability\n",
    "num_episodes = 100"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "330323d2a1323109"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project = 'RL_Drug_Generation',\n",
    "    name= f'',\n",
    "    config={\n",
    "        'lr': lr,\n",
    "        'episodes': num_episodes,\n",
    "        'gamma': gamma,\n",
    "        'num_envs': num_envs\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23695977438a2cf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(num_episodes):\n",
    "\n",
    "    # Reset environment after each episode\n",
    "    states = env.reset()\n",
    "\n",
    "    # Episode loggers\n",
    "    saved_actions = []\n",
    "    saved_log_probs = []\n",
    "    saved_rewards = []\n",
    "    saved_mol_sizes = []\n",
    "\n",
    "    # Episode computation\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        # Compute actions and log probabilities\n",
    "        batch = batch_from_states(states)\n",
    "        actions, log_probs = model.act(batch, return_log_probs = True)\n",
    "\n",
    "        # Take a step in environment\n",
    "        states, rewards, valids, timestep = env.step(actions['t'], actions['nmol'], actions['nfull'], actions['b'], version = 't')\n",
    "\n",
    "        # Record in episode loggers\n",
    "        if step == 0:\n",
    "            saved_mol_sizes = env.mol_sizes\n",
    "            saved_actions = actions['t']\n",
    "            saved_log_probs = log_probs['t']\n",
    "            saved_rewards = torch.tensor(rewards)\n",
    "        else:\n",
    "            saved_mol_sizes = np.vstack((saved_mol_sizes, env.mol_sizes))\n",
    "            saved_actions = np.vstack((saved_actions, actions['t']))\n",
    "            saved_log_probs = torch.vstack((saved_log_probs, log_probs['t']))\n",
    "            saved_rewards = torch.vstack((saved_rewards, torch.tensor(rewards)))\n",
    "\n",
    "    # 3. Loss Calculation & Gradient Ascent\n",
    "    cumulative_reward = torch.sum(saved_rewards) / num_envs\n",
    "\n",
    "    # Calculate returns\n",
    "    all_returns = torch.tensor(num_envs)\n",
    "    returns = torch.zeros(num_envs)\n",
    "    for idx in reversed(range(max_steps)):\n",
    "        returns = saved_rewards[idx, :] + gamma * returns\n",
    "        if idx == max_steps - 1:\n",
    "            all_returns = returns\n",
    "        else:\n",
    "            all_returns = torch.vstack((returns, all_returns))\n",
    "    all_returns = (all_returns - all_returns.mean()) / (all_returns.std() + eps)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = torch.sum(-1 * all_returns * saved_log_probs / num_envs)\n",
    "\n",
    "    # Perform gradient ascent on cumulative reward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({\"Cumulative Reward\": cumulative_reward, \"Loss\": loss, \"Average Size\": saved_mol_sizes.mean()})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8041324c61c44f38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7c57457bd175a8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
