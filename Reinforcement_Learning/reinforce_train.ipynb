{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-23T00:36:30.084109Z",
     "start_time": "2023-10-23T00:36:30.013321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from Model.SURGE import SURGE\n",
    "from Reinforcement_Learning.mol_env import vectorized_mol_env\n",
    "from Model.graph_embedding import batch_from_states\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "import wandb\n",
    "import copy\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit.Chem.QED as QED # QED\n",
    "import sys # SAS\n",
    "import os\n",
    "from rdkit.Chem import RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "from rdkit.Chem import Descriptors # MW\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "model = SURGE()\n",
    "max_steps = 200\n",
    "num_envs = 4\n",
    "env = vectorized_mol_env(num_envs = num_envs, max_steps = max_steps) # Vectorized molecular environment\n",
    "lr = 0.02\n",
    "optimizer = Adam(lr = lr, params = model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T00:36:31.174292Z",
     "start_time": "2023-10-23T00:36:31.118626Z"
    }
   },
   "id": "7f32500a4953f295"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "eps = np.finfo(np.float32).eps.item() # Small constant to decrease numerical instability\n",
    "num_episodes = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T00:34:59.646921Z",
     "start_time": "2023-10-23T00:34:59.596709Z"
    }
   },
   "id": "5ad436df1363c9c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_datetime = datetime.datetime.now()\n",
    "current_time = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "wandb.init(\n",
    "    project = 'RL_Drug_Generation',\n",
    "    name= f'REINFORCE---{current_time}',\n",
    "    config={\n",
    "        'lr': lr,\n",
    "        'architecture': str(model),\n",
    "        'episodes': num_episodes,\n",
    "        'gamma': gamma,\n",
    "        'num_envs': num_envs,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4af6d57c08a0eaec"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def evaluate(saved_states, saved_actions):\n",
    "    \"\"\"\n",
    "    This methods evaluates the molecules generated during the episode and returns diversity, novelty, and validity.\n",
    "    It also analyzes the molecule metrics of drug-likeness (QED), synthetic accessibility (SAS), and size.\n",
    "    It only analyzes the finished molecules (i.e., after SURGE tells generation to terminate).\n",
    "    \"\"\"\n",
    "    \n",
    "    if 1 in saved_actions['t']:\n",
    "        flattened_states = []\n",
    "        flattened_t = []\n",
    "        for i in range(saved_states.shape[1]):\n",
    "            states_col = saved_states[:, i].flatten().tolist()\n",
    "            t_col = saved_actions['t'][:, i].flatten().tolist()\n",
    "            if i == 0:\n",
    "                flattened_states = states_col\n",
    "                flattened_t = t_col\n",
    "            else:\n",
    "                flattened_states += states_col\n",
    "                flattened_t += t_col\n",
    "        \n",
    "        idx = [i for i, t in enumerate(flattened_t) if t == 1]\n",
    "        finished_states = [flattened_states[idx[i]] for i in range(len(idx))]\n",
    "    else:\n",
    "        finished_states = [saved_states[i, saved_states.shape[1] - 1] for i in range(saved_states.shape[0])]\n",
    "    \n",
    "    \n",
    "    # QED: From 0 to 1 where 1 is the most drug-like\n",
    "    # SAS: From 1 to 10 where 1 is the easiest to synthesize\n",
    "    metric_names = ['Average Size', 'Average MW', 'Average QED', 'Average SAS']\n",
    "    num_mols = len(finished_states)\n",
    "    metrics = dict()\n",
    "    for name in metric_names:\n",
    "        metrics[name] = 0\n",
    "        \n",
    "    for i in range(num_mols):\n",
    "        finished_states[i].UpdatePropertyCache()\n",
    "        if finished_states[i].GetNumAtoms() == 1:\n",
    "            metrics['Average SAS'] = 0\n",
    "        else:\n",
    "            metrics['Average SAS'] += sascorer.calculateScore(finished_states[i])\n",
    "        \n",
    "        metrics['Average Size'] += finished_states[i].GetNumAtoms()\n",
    "        metrics['Average MW'] += Descriptors.MolWt(finished_states[i])\n",
    "        metrics['Average QED'] += QED.weights_mean(finished_states[i])\n",
    "    \n",
    "    for name in metric_names:\n",
    "        metrics[name] /= num_mols\n",
    "    \n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T00:35:01.185775Z",
     "start_time": "2023-10-23T00:35:01.131343Z"
    }
   },
   "id": "129d2caf1577df9e"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def calc_returns(saved_rewards, gamma):\n",
    "    \"\"\"\n",
    "    Given multidimensional matrix of rewards, computes the returns.\n",
    "    \"\"\"\n",
    "    prev_row = []\n",
    "    for reward_idx in reversed(range(saved_rewards.shape[0])):\n",
    "        cur_row = saved_rewards[reward_idx, :]\n",
    "        if reward_idx == saved_rewards.shape[0] - 1:\n",
    "            all_returns = cur_row\n",
    "            prev_row = cur_row\n",
    "        else:\n",
    "            all_returns = torch.vstack((prev_row * gamma + cur_row, all_returns))\n",
    "            prev_row *= gamma\n",
    "            prev_row += cur_row\n",
    "    return all_returns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T00:35:01.442615Z",
     "start_time": "2023-10-23T00:35:01.392550Z"
    }
   },
   "id": "d259874b09dfefef"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 200/200 [00:02<00:00, 95.59steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 1.5012345679012347\n",
      "Average MW: 29.4209679012344\n",
      "Average QED: 0.3578981877099945\n",
      "Average SAS: 0.0\n",
      "Cumulative Reward: -416.8781498549757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 2: 100%|██████████| 200/200 [00:02<00:00, 94.79steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 1.8773946360153257\n",
      "Average MW: 39.06759770114932\n",
      "Average QED: 0.35473127564372364\n",
      "Average SAS: 0.09885798123236081\n",
      "Cumulative Reward: -201.08150524561486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3: 100%|██████████| 200/200 [00:02<00:00, 98.34steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 2.5307692307692307\n",
      "Average MW: 51.04302307692306\n",
      "Average QED: 0.34516120807285156\n",
      "Average SAS: 0.0\n",
      "Cumulative Reward: -104.81753391900634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 4: 100%|██████████| 200/200 [00:02<00:00, 97.20steps/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.109090909090909\n",
      "Average MW: 78.81438181818181\n",
      "Average QED: 0.32221684034497455\n",
      "Average SAS: 0.31355702741897434\n",
      "Cumulative Reward: -64.83104561805558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 5: 100%|██████████| 200/200 [00:02<00:00, 96.87steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.303030303030303\n",
      "Average MW: 90.6930303030303\n",
      "Average QED: 0.338750895284795\n",
      "Average SAS: 1.1075870925456364\n",
      "Cumulative Reward: -77.81102909334112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 6: 100%|██████████| 200/200 [00:02<00:00, 86.97steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.25\n",
      "Average MW: 83.96845\n",
      "Average QED: 0.33093793066466665\n",
      "Average SAS: 4.519265414346751\n",
      "Cumulative Reward: -84.99653052795418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 7: 100%|██████████| 200/200 [00:02<00:00, 97.56steps/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.615384615384615\n",
      "Average MW: 77.107\n",
      "Average QED: 0.3306032465682463\n",
      "Average SAS: 5.078762521712268\n",
      "Cumulative Reward: -91.81118810718903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 8: 100%|██████████| 200/200 [00:02<00:00, 98.93steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.0625\n",
      "Average MW: 76.69731249999998\n",
      "Average QED: 0.35823040080229934\n",
      "Average SAS: 4.940507059437719\n",
      "Cumulative Reward: -91.706116395392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 9: 100%|██████████| 200/200 [00:02<00:00, 98.30steps/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 3.619047619047619\n",
      "Average MW: 63.9227619047619\n",
      "Average QED: 0.33530403937405356\n",
      "Average SAS: 2.123123045733815\n",
      "Cumulative Reward: -84.96850548924999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 10: 100%|██████████| 200/200 [00:01<00:00, 101.23steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.555555555555555\n",
      "Average MW: 77.69155555555554\n",
      "Average QED: 0.3278292431335518\n",
      "Average SAS: 5.052902429775141\n",
      "Cumulative Reward: -93.45929168123837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 11: 100%|██████████| 200/200 [00:01<00:00, 101.07steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.066666666666666\n",
      "Average MW: 80.62633333333335\n",
      "Average QED: 0.3313980532414583\n",
      "Average SAS: 5.136969167698374\n",
      "Cumulative Reward: -92.58480128198492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 12: 100%|██████████| 200/200 [00:02<00:00, 96.28steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.9375\n",
      "Average MW: 96.49931249999999\n",
      "Average QED: 0.3538122486371012\n",
      "Average SAS: 0.3601086519427103\n",
      "Cumulative Reward: -82.10014477729442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 13: 100%|██████████| 200/200 [00:02<00:00, 92.82steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 9.2\n",
      "Average MW: 218.53359999999998\n",
      "Average QED: 0.3564327378875797\n",
      "Average SAS: 5.758277306011797\n",
      "Cumulative Reward: -84.05821883089297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 14: 100%|██████████| 200/200 [00:02<00:00, 93.77steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 5.411764705882353\n",
      "Average MW: 130.6599411764706\n",
      "Average QED: 0.32091731376633864\n",
      "Average SAS: 3.304898383277883\n",
      "Cumulative Reward: -81.67743651954675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 15: 100%|██████████| 200/200 [00:02<00:00, 84.08steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 7.857142857142857\n",
      "Average MW: 150.51742857142858\n",
      "Average QED: 0.3054636077185177\n",
      "Average SAS: 7.164425593980437\n",
      "Cumulative Reward: -85.46916970296645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 16: 100%|██████████| 200/200 [00:02<00:00, 93.69steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 5.666666666666667\n",
      "Average MW: 106.93758333333335\n",
      "Average QED: 0.3498054950195361\n",
      "Average SAS: 0.8640669565587297\n",
      "Cumulative Reward: -72.43128276744584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 17: 100%|██████████| 200/200 [00:02<00:00, 94.94steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.833333333333333\n",
      "Average MW: 84.74536666666667\n",
      "Average QED: 0.31076723390974303\n",
      "Average SAS: 1.2509338789765354\n",
      "Cumulative Reward: -72.72614519210968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 18: 100%|██████████| 200/200 [00:02<00:00, 96.71steps/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Size: 4.67741935483871\n",
      "Average MW: 77.51422580645162\n",
      "Average QED: 0.3395989273161674\n",
      "Average SAS: 1.0174124222847882\n",
      "Cumulative Reward: -76.40215435275621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 19:  17%|█▋        | 34/200 [00:00<00:01, 91.90steps/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[107], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m pbar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Compute actions and log probabilities\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_from_states\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m actions, log_probs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mact(batch)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Record in episode loggers\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/RL_Drug_Generation/Model/graph_embedding.py:157\u001B[0m, in \u001B[0;36mbatch_from_states\u001B[0;34m(states)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;124;03mAccepts a list of RWMol objects. Outputs a batch of embedded graphs with atom bank.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    156\u001B[0m smiles \u001B[38;5;241m=\u001B[39m [Chem\u001B[38;5;241m.\u001B[39mMolToSmiles(state) \u001B[38;5;28;01mfor\u001B[39;00m state \u001B[38;5;129;01min\u001B[39;00m states]\n\u001B[0;32m--> 157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbatch_from_smiles\u001B[49m\u001B[43m(\u001B[49m\u001B[43msmiles\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RL_Drug_Generation/Model/graph_embedding.py:132\u001B[0m, in \u001B[0;36mbatch_from_smiles\u001B[0;34m(all_smiles)\u001B[0m\n\u001B[1;32m    129\u001B[0m X \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mvstack((X, atom_bank))\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m# construct edge index array E of shape (2, n_edges)\u001B[39;00m\n\u001B[0;32m--> 132\u001B[0m (rows, cols) \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mnonzero(Chem\u001B[38;5;241m.\u001B[39mGetAdjacencyMatrix(mol))\n\u001B[1;32m    133\u001B[0m torch_rows \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(rows\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mint64))\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m    134\u001B[0m torch_cols \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(cols\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mint64))\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mlong)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_reward_model = copy.deepcopy(model)\n",
    "best_reward = 0\n",
    "best_episode = 0\n",
    "for episode in range(1, num_episodes + 1):\n",
    "\n",
    "    # Reset environment after each episode\n",
    "    states = env.reset()\n",
    "\n",
    "    # Episode loggers\n",
    "    keys = ['t', 'nmol', 'nfull', 'b']\n",
    "    saved_actions = {'t': [], 'nmol': [], 'nfull': [], 'b': []}\n",
    "    saved_log_probs = {'t': [], 'nmol': [], 'nfull': [], 'b': []}\n",
    "    saved_rewards = []\n",
    "    saved_states = np.array(states, dtype = object)\n",
    "\n",
    "    # Episode computation\n",
    "    pbar = trange(max_steps, unit=\"steps\")\n",
    "    for step in pbar:\n",
    "        pbar.set_description(f\"Episode {episode}\")\n",
    "        \n",
    "        # Compute actions and log probabilities\n",
    "        batch = batch_from_states(states)\n",
    "        actions, log_probs = model.act(batch)\n",
    "\n",
    "        # Record in episode loggers\n",
    "        for key in keys:\n",
    "            if step == 0:\n",
    "                saved_actions[key] = actions[key]\n",
    "                saved_log_probs[key] = log_probs[key]\n",
    "            else:\n",
    "                saved_actions[key] = np.vstack((saved_actions[key], actions[key]))\n",
    "                saved_log_probs[key] = torch.vstack((saved_log_probs[key], log_probs[key]))\n",
    "\n",
    "        # Take a step in environment\n",
    "        states, rewards, valids, timestep = env.step(actions['t'], actions['nmol'], actions['nfull'], actions['b'])\n",
    "        saved_states = np.vstack((saved_states, states))\n",
    "\n",
    "        # Record rewards\n",
    "        if step == 0:\n",
    "            saved_rewards = torch.tensor(rewards)\n",
    "        else:\n",
    "            saved_rewards = torch.vstack((saved_rewards, torch.tensor(rewards)))\n",
    "\n",
    "    # Loss calculation and gradient ascent\n",
    "    cumulative_reward = torch.sum(saved_rewards) / num_envs\n",
    "    \n",
    "    metrics = evaluate(saved_states, saved_actions)\n",
    "    \n",
    "    # Returns\n",
    "    all_returns = calc_returns(saved_rewards, gamma)\n",
    "    returns = torch.zeros(num_envs)\n",
    "    all_returns = (all_returns - all_returns.mean()) / (all_returns.std() + eps) # Normalize returns for better stability\n",
    "    \n",
    "    # Calculate loss\n",
    "    all_loss = dict()\n",
    "    cumulative_loss = 0\n",
    "    for key in keys:\n",
    "        # Find the average loss among vectorized environments for each SURGE component\n",
    "        individual_loss = -1 * all_returns * saved_log_probs[key] / num_envs\n",
    "        cumulative_loss += torch.sum(individual_loss)\n",
    "        all_loss[key] = torch.sum(individual_loss)\n",
    "\n",
    "    # Perform gradient ascent\n",
    "    optimizer.zero_grad()\n",
    "    cumulative_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    metrics = evaluate(saved_states, saved_actions)\n",
    "    metrics['Cumulative Reward'] = cumulative_reward.item()\n",
    "    \n",
    "    for key in metrics.keys():\n",
    "        print(f\"{key}: {metrics[key]}\")\n",
    "    \n",
    "    metrics['Cumulative Loss'] = cumulative_loss.item()\n",
    "    metrics['Nmol Loss'] = all_loss['nmol'].item()\n",
    "    metrics['Nfull Loss'] = all_loss['nfull'].item()\n",
    "    metrics['Bond Loss'] = all_loss['b'].item()\n",
    "    metrics['Termination Loss'] = all_loss['t'].item()\n",
    "    \n",
    "    if cumulative_reward > best_reward:\n",
    "        best_reward = cumulative_reward\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_episode = episode\n",
    "    \n",
    "    # wandb.log(metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T00:37:23.326565Z",
     "start_time": "2023-10-23T00:36:33.733870Z"
    }
   },
   "id": "5bda2bbdddefc344"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778c374f34d1064c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = vectorized_mol_env()\n",
    "best_model.eval()\n",
    "states = env.reset()\n",
    "print(best_model.act(batch_from_states(states)))\n",
    "for i in range(200):\n",
    "    actions = best_model.act(batch_from_states(states))\n",
    "    states, rewards, valids, timestep = env.step(actions['t'], actions['nmol'], actions['nfull'], actions['b'])\n",
    "    print(f'{i}:\\t{rewards}\\t{states}')\n",
    "    if i % 200 == 0:\n",
    "        states = env.reset()\n",
    "    env.visualize()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35ff5c58913d2371"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
